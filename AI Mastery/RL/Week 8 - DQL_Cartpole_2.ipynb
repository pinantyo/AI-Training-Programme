{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/oektomo/KelasRLG2/blob/master/DQL_Cartpole_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IQd00eaaf53-"},"outputs":[],"source":["# !pip install score\n","# https://gsurma.medium.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-dP9nwjVGmn"},"outputs":[],"source":["import random\n","import gym\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import deque\n","from keras.models import Sequential\n","from keras.layers import Dense\n","#from keras.optimizers import Adam\n","from tensorflow.keras.optimizers import Adam\n","\n","# from scores.score_logger import ScoreLogger\n","\n","ENV_NAME = \"CartPole-v1\"\n","\n","GAMMA = 0.95\n","LEARNING_RATE = 0.001\n","\n","MEMORY_SIZE = 1000000\n","BATCH_SIZE = 20\n","\n","EXPLORATION_MAX = 1.0 #epsilon max\n","EXPLORATION_MIN = 0.01 #epsilon min\n","EXPLORATION_DECAY = 0.995"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRLxoXtdVefh"},"outputs":[],"source":["#Class dari NN atau memori\n","\n","class DQNSolver: \n","  def __init__(self, observation_space, action_space):\n","    self.exploration_rate = EXPLORATION_MAX\n","    \n","    self.action_space = action_space\n","    self.memory = deque(maxlen=MEMORY_SIZE)\n","\n","    self.model = Sequential()\n","    self.model.add(Dense(24, input_shape=(observation_space, ), activation=\"relu\"))\n","    self.model.add(Dense(24, activation=\"relu\"))\n","    self.model.add(Dense(self.action_space, activation=\"linear\"))\n","    # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","    self.model.compile(loss=\"mse\", optimizer=Adam(learning_rate=LEARNING_RATE))\n","  \n","  def remember(self, state, action, reward, next_state, done):\n","    self.memory.append((state, action, reward, next_state, done))\n","\n","  # choose if we want to explore or use the neural net\n","  def act(self, state):\n","    if np.random.rand() < self.exploration_rate:\n","      return random.randrange(self.action_space)\n","    q_values = self.model.predict(state)\n","    return np.argmax(q_values[0])\n","\n","  # Replay Memory in Experience Replay\n","  def experience_replay(self):\n","    if len(self.memory) < BATCH_SIZE:\n","      return\n","    batch = random.sample(self.memory, BATCH_SIZE)\n","    for state, action, reward, state_next, terminal in batch:\n","      q_update = reward\n","      if not terminal:\n","        q_update = reward + GAMMA * np.amax(self.model.predict(state_next)[0])\n","      q_values = self.model.predict(state)\n","      q_values[0][action] = q_update\n","      self.model.fit(state, q_values, verbose=0)\n","    self.exploration_rate *= EXPLORATION_DECAY\n","    self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBqbZjUEe3Io"},"outputs":[],"source":["def cartpole(max_run = 3):\n","    env = gym.make(ENV_NAME)\n","    #score_logger = ScoreLogger(ENV_NAME)\n","    observation_space = env.observation_space.shape[0]\n","    action_space = env.action_space.n\n","    dqn_solver = DQNSolver(observation_space, action_space)\n","    run = 0\n","    score = []\n","    lcpos = [] # longest episode cart position\n","    lcvel = [] # longest episode cart velocity\n","    lpangl = [] # longest episode pole angle\n","    #max_run = 3\n","    while True:\n","        run += 1\n","        state = env.reset()\n","        state = np.reshape(state, [1, observation_space])\n","        step = 0\n","        cpos = []\n","        cvel = []\n","        pangl = []\n","        cpos.append(state[0,0])\n","        cvel.append(state[0,1])\n","        pangl.append(state[0,2])\n","        while True:\n","            step += 1\n","            #env.render()\n","            action = dqn_solver.act(state)\n","            state_next, reward, terminal, info = env.step(action)\n","            cpos.append(state_next[0])\n","            cvel.append(state_next[1])\n","            pangl.append(math.degrees(state_next[2]))\n","            reward = reward if not terminal else -reward\n","            state_next = np.reshape(state_next, [1, observation_space])\n","            dqn_solver.remember(state, action, reward, state_next, terminal)\n","            state = state_next\n","            if terminal:\n","                print (\"Episode: \"+str(run)+\n","                       \", exploration: \"+str(round(dqn_solver.exploration_rate, 3))+\n","                       \", score: \"+str(step)+\n","                       \", pos: \"+str(round(state_next[0,0], 3))+\n","                       \", angle: \"+str(round(math.degrees(state_next[0,2]), 2))+\n","                       \" deg\")\n","                score.append(step)\n","                #if cpos.size > lcpos.size:\n","                if len(cpos) > len(lcpos):\n","                    lcpos = cpos\n","                    lcvel = cvel\n","                    lpangl = pangl\n","                #plt.plot(pangl)\n","                #plt.plot(cvel)\n","                #score_logger.add_score(step, run)\n","                break\n","            dqn_solver.experience_replay()\n","        if run >= max_run:\n","            break\n","    plt.plot(score)\n","    return score, lcpos, lcvel, lpangl\n","    # todo: check the code for saving the longest episode cart speed, cart position, pole angle\n","    # refactor the code to make one list to record angle and score?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"PxZvz9En7hhA","outputId":"b99fbb0b-8893-4d31-beb6-c2357b5fe876"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 1, exploration: 0.97, score: 26, pos: -0.13, angle: 12.15 deg\n","Episode: 2, exploration: 0.896, score: 17, pos: -0.194, angle: 12.93 deg\n","Episode: 3, exploration: 0.856, score: 10, pos: -0.122, angle: 14.44 deg\n","Episode: 4, exploration: 0.806, score: 13, pos: -0.058, angle: 13.3 deg\n","Episode: 5, exploration: 0.715, score: 25, pos: -0.117, angle: 12.68 deg\n","Episode: 6, exploration: 0.676, score: 12, pos: -0.118, angle: 13.0 deg\n","Episode: 7, exploration: 0.634, score: 14, pos: -0.12, angle: 13.22 deg\n","Episode: 8, exploration: 0.579, score: 19, pos: -0.171, angle: 12.56 deg\n","Episode: 9, exploration: 0.551, score: 11, pos: 0.195, angle: -12.19 deg\n","Episode: 10, exploration: 0.526, score: 10, pos: 0.068, angle: -13.24 deg\n"]}],"source":["score, lcpos, lcvel, lpangl = cartpole(100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IdogEvndgUX0"},"outputs":[],"source":["plt.plot(score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcAz7Y5kO5ch"},"outputs":[],"source":["plt.plot(lcpos)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynd8OWK-gWfr"},"outputs":[],"source":["env.observation_space.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXPfB9sYgZSM"},"outputs":[],"source":["env.action_space"]},{"cell_type":"markdown","metadata":{"id":"bsxHoE2thI4v"},"source":["\n","    Observation: \n","        Type: Box(4)\n","\n","    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n","    | Num | Observation           | Min                  | Max                |\n","    |-----|-----------------------|----------------------|--------------------|\n","    | 0   | Cart Position         | -4.8                 | 4.8                |\n","    | 1   | Cart Velocity         | -Inf                 | Inf                |\n","    | 2   | Pole Angle            | ~ -0.418 rad (-24°)  | ~ 0.418 rad (24°)  |\n","    | 3   | Pole Angular Velocity | -Inf                 | Inf                |\n","    **Note:** While the ranges above denote the possible values for observation space of each element, \n","    it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n","    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates if the cart leaves the `(-2.4, 2.4)` range.\n","    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n","\n","\n","    Action:\n","        Type: Discrete(2)\n","        Num\tAction\n","        0\tPush cart to the left\n","        1\tPush cart to the right"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AX1NPGC7Pd5U"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of DQL_Cartpole_2.ipynb","provenance":[{"file_id":"1g067SfnqYkaAknz1EVIPv-IIzxO-UR3G","timestamp":1650200244868}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}