{"cells":[{"cell_type":"markdown","metadata":{"id":"pXc3JfkT9x1E"},"source":["# Module 4\n","## Section: Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"XKxWI_6y9x1K"},"source":["## Lab 2: Logistic Regression and Evaluation Metrics"]},{"cell_type":"markdown","metadata":{"id":"xkQqVkb19x1L"},"source":["<br><br><br><br>\n","## Objective\n","***\n","- Approach\n","- Logistic Regression\n","    - Cost Function\n","    - Gradient Descent\n","    - Evaluation Metrics for Logistic Regression\n","        - Confusion Matrix\n","        - Precision and Recall\n","        - F-1 score\n","        - Area under ROC curve\n","        - Logarithmic Loss"]},{"cell_type":"markdown","metadata":{"id":"z5Hbc6_K9x1L"},"source":["<br><br><br><br>\n","## Approach\n","***\n","- After learning basics, lets see how the Loan Prediction data set could use the same techniques\n","- We want further insight to what this data set looks like and how we would go about implementing this \n","- It would be smart if we split this data set into a *Training Set* and *Test Set* \n","\n","    - I'll leave it to you to figure out why this would be appropriate"]},{"cell_type":"markdown","metadata":{"id":"BJfACxrc9x1M"},"source":["<br><br><br><br>\n","## Logistic Regression - Cost Function and Gradient Descent\n","***\n","- Till now we studied the intuition behind the Sigmoid Function\n","\n","- We also studied how Logistic Regression works to get outputs in the range of [0,1]\n","\n","- We discussed the interpretation of the output too! "]},{"cell_type":"markdown","metadata":{"id":"L1G1DDJC9x1N"},"source":["## Cost Function \n","***\n"," - Fit θ parameters\n"," - Define the optimization object for the cost function we use to fit the parameters\n","     - Training set consists of **\"m\"** training examples\n","         - Each example has a **n+1** length column vector\n","***      \n"," \n","<center>Training set:  $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})\\}$<br/><br/></center>\n","<center>m examples $x \\epsilon \\begin{bmatrix}x_{0}\\\\x_{1}\\\\\\cdots\\\\x_{n}\\end{bmatrix}$</center><center>$x_{0}=1,y\\epsilon \\{0,1\\}$</center> \n","\n"," $$ h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^TX}} $$\n"," \n"," ***\n","* This is the situation: \n","  - Set of m training examples\n","  - Each example is a feature vector which is n+1 dimensional\n","  - $x_0$ = 1\n","  - y ∈ {0,1}\n","  - Hypothesis is based on parameters (θ)\n","      - **Given the training set how to we chose/fit θ?**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fdNBFlav9x1O"},"source":["Our cost function for \"m\" training examples is: \n","***\n","  $$ J(\\theta) =- \\frac{1}{m}[\\sum_{i=1}^my^{(i)}logh_\\theta(x^{(i)})+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))] $$"]},{"cell_type":"markdown","metadata":{"id":"_LBKgJlu9x1O"},"source":["## Gradient Descent\n","***\n"," <center>Repeat for all $\\theta_j$ simultaneously { <br> $\\theta_j := \\theta_j - \\alpha\\sum_{i=1}^m(h_{\\theta}(x^{(i)})- y^{(i)})x_{j}^{(i)}$</center>\n"," <center>}\n","\n"," ---\n"," alfa = learning rate\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Ht3w5AD9x1P"},"outputs":[],"source":["# Importing Libraries\n","import pandas as pd\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.pipeline import Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zt4hhItt9x1Q"},"outputs":[],"source":["#Splitting Data\n","# download dataset from - \n","# https://raw.githubusercontent.com/bluedataconsulting/AIMasteryProgram/main/Lab_Exercises/Module4/loan_prediction.csv\n","dataframe = pd.read_csv('loan_prediction.csv')\n","X = dataframe.iloc[:,:-1]\n","y = dataframe.iloc[:,-1]\n","\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgfHYtOD9x1R"},"outputs":[],"source":["# Training a logistic regression model\n","logistic_regressor = LogisticRegression(max_iter=1000)\n","pipeline = Pipeline(steps=[('add_poly_features', PolynomialFeatures()),\n","                           ('logistic_regression', logistic_regressor)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwBdN33k9x1S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647392277624,"user_tz":-420,"elapsed":1049,"user":{"displayName":"Muhammad Afif Baihaqi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqRNXBjwvL526IDkKtKbePkBR16jH3pGktAQ_jyw=s64","userId":"18398307286314040811"}},"outputId":"de10dca5-125b-4acc-aa99-1295cce4fa76"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"]}],"source":["pipeline.fit(X_train, y_train)\n","y_pred = pipeline.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"AMKFf8AL9x1S"},"source":["<br><br><br><br>\n","## Evalution Metrics for Logistic Regression\n","***\n","\n","* As we already know, we use different metrics for regression and classification\n","* We know that we can use `MSE` for regression problems and `Accuracy` for classification problems\n","* However, these might not be the best metrics in every situation<br><br>"]},{"cell_type":"markdown","metadata":{"id":"pdEwRD0v9x1T"},"source":["## Evalution Metrics for Logistic Regression\n","***\n","\n","* Following are the types of Classification Metrics :\n","    * Confusion Matrix\n","    * Classification Matrix\n","    * F1 Score\n","    * Area under ROC curve\n","    * Classification Report\n","    * Logarithmic Loss"]},{"cell_type":"markdown","metadata":{"id":"lkPUbOHB9x1T"},"source":["<br><br><br><br>\n","\n","### Confusion Matrix\n","***\n","- The confusion matrix is a handy presentation of the accuracy of a model with two or more classes. Below is an example of a Confusion Matrix \n","<br><br>\n","\n","\n","| Value | Fraud  | Not Fraud |\n","|---|---|---|\n","| Predicted Fraud | 1 | 1 |\n","| Predicted Not Fraud | 2 | 996 |\n","\n","\n","    True Positives (TP): These are predicted yes and actually yes (Top Left)\n","    True Negatives (TN): We predicted no, and actually no (Top Right) \n","    False Positives (FP): We predicted yes, but actually no. (AKA \"Type I error.\") (Top Right) \n","    False Negatives (FN): We predicted no, but yes. (AKA\"Type II error.\") (Bottom Left)\n"]},{"cell_type":"markdown","metadata":{"id":"eGaIq94C9x1T"},"source":["### Confusion Matrix\n","***\n","* Classification accuracy is the number of correct predictions **(TN + TP)** made as a ratio of all predictions made. **(TN + TP +FN + FP)**<br><br>\n","It is suitable when :\n","* There are an equal number of observations in each class\n","* That all predictions and prediction errors are equally important,which is often not the case."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4W9liY29x1T","outputId":"65482119-d673-4cee-994a-e63ccb330955","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647392277624,"user_tz":-420,"elapsed":10,"user":{"displayName":"Muhammad Afif Baihaqi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqRNXBjwvL526IDkKtKbePkBR16jH3pGktAQ_jyw=s64","userId":"18398307286314040811"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 10,  46],\n","       [  9, 120]])"]},"metadata":{},"execution_count":7}],"source":["# Applying confusion matrix on above data\n","from sklearn.metrics import confusion_matrix\n","confusion_matrix(y_test,y_pred)"]},{"cell_type":"markdown","metadata":{"id":"-gAg_0mi9x1U"},"source":["### Precision\n","***\n","\n","$$Precision = \\frac {(True +ves)} {(True +ves  +  False +ves)}$$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifrlXBwI9x1V","outputId":"077f3094-b0f3-4845-dfc8-a04d9eb644f8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647392277624,"user_tz":-420,"elapsed":8,"user":{"displayName":"Muhammad Afif Baihaqi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqRNXBjwvL526IDkKtKbePkBR16jH3pGktAQ_jyw=s64","userId":"18398307286314040811"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7228915662650602"]},"metadata":{},"execution_count":8}],"source":["from sklearn.metrics import confusion_matrix, precision_score, recall_score\n","precision_score(y_test,y_pred)"]},{"cell_type":"markdown","metadata":{"id":"tz6kZZsk9x1V"},"source":["### Recall \n","***\n","\n","$$Recall = \\frac {(True +ves)} {(True +ves  +  False -ves)}$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcN9HeDi9x1V","outputId":"b91428f7-8919-4342-8d52-ed5b9fa8afb4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647392277624,"user_tz":-420,"elapsed":7,"user":{"displayName":"Muhammad Afif Baihaqi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqRNXBjwvL526IDkKtKbePkBR16jH3pGktAQ_jyw=s64","userId":"18398307286314040811"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9302325581395349"]},"metadata":{},"execution_count":9}],"source":["recall_score(y_test,y_pred)"]},{"cell_type":"markdown","metadata":{"id":"YldmHp7g9x1W"},"source":["## Trade - Off: Precision Vs. Recall \n","***\n","- This is more of a in-class activity! \n","- Think about this: What happens if we get an increased value of Precision? Do you think that would lower Recall? And vice-versa? \n","\n","- Think of an example! And use easy numerical calculations too. You can just use a pencil and paper, no need for code! \n","\n","- [**Hint**: There is a trade-off!] "]},{"cell_type":"markdown","metadata":{"id":"rNWGrUDE9x1W"},"source":["### F1 Score\n","***\n"," - To deal with this Trade-off we calculate something known as the F-1 Score: F1 score is a good approach to minimize a bias towards either the Precision or the Recall\n","\n"," $$F1 Score = \\frac {2PR} {P + R} $$"]},{"cell_type":"markdown","metadata":{"id":"3ymuRVMY9x1W"},"source":["***\n","F1 Score is defined as \n","\n","<center>$2*\\frac{precision*recall}{precision+recall}$</center>\n","\n","* tp = true positive\n","* tn = true negative\n","* fp = false positive\n","* fn = false negative"]},{"cell_type":"markdown","metadata":{"id":"oOWeSOM89x1W"},"source":["\n","***\n","- Using this intuition, we want to calculate the F-1 Score to better understand the evaluation of our model\n","\n","- Let's see how to implement this in Python! "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pcUkXmCn9x1X","outputId":"f85bbece-33c2-463f-a475-dfb2723ecce7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647392277625,"user_tz":-420,"elapsed":7,"user":{"displayName":"Muhammad Afif Baihaqi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqRNXBjwvL526IDkKtKbePkBR16jH3pGktAQ_jyw=s64","userId":"18398307286314040811"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8135593220338984"]},"metadata":{},"execution_count":10}],"source":["## code for f-1 score\n","\n","from sklearn.metrics import f1_score\n","f1_score(y_test,y_pred)"]},{"cell_type":"markdown","metadata":{"id":"abMpp9Ir9x1Y"},"source":["## Logarithmic Loss\n","***\n","Logarithmic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of membership to a given class\n","\n","\n","Where,\n","* N is the number of samples or instances,\n","* M is the number of possible labels,\n","* y<sub>ij</sub> is a binary indicator of whether or not label j is the correct classification for instance i,\n","* p<sub>ij</sub> is the model probability of assigning label j to instance i.\n","\n"," $$ Logloss=- \\frac{1}{n}\\sum_{i=1}^n[y_ilogp_{i}+(1-y_{i})log(1-p_{i})] $$\n"]},{"cell_type":"markdown","metadata":{"id":"sDsMLK2-9x1Y"},"source":["\n","***\n","* The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm.<br>\n","* Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.<br>\n","* logloss nearer to 0 is better, with 0 representing a perfect logloss. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZBA7VXW9x1Z","outputId":"252885a6-e82d-4577-a95e-fb7c91e45552","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647392277625,"user_tz":-420,"elapsed":5,"user":{"displayName":"Muhammad Afif Baihaqi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhqRNXBjwvL526IDkKtKbePkBR16jH3pGktAQ_jyw=s64","userId":"18398307286314040811"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["10.268483692983116"]},"metadata":{},"execution_count":12}],"source":["from sklearn.metrics import log_loss\n","log_loss(y_test,y_pred)"]},{"cell_type":"markdown","metadata":{"id":"z-eVxz1n9x1Z"},"source":["### Thank you!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYPFIHbB9x1Z"},"outputs":[],"source":[""]}],"metadata":{"anaconda-cloud":{},"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"16Maret-Module4_Section2_Lab2_Logistic_Regression.ipynb","provenance":[{"file_id":"1TlYoJjQ1zOexOHEuNwP5W83H3G06j50i","timestamp":1647403744410},{"file_id":"1SlY7233IRxjjQ_WgFKKHmeeoIMSt9Yfk","timestamp":1647392087551}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}